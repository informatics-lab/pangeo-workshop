{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/dask_logo.svg\" alt=\"Dask\" style=\"height: 250px;\"/> \n",
    "\n",
    "# [Dask](https://docs.dask.org/en/latest/)\n",
    "\n",
    "Dask is a Python library which allows large data processing tasks to be automatically broken up into chunks and executed on a cluster of workers.\n",
    "\n",
    "Dask is integrated with many Python tools used for data processing such as:\n",
    "- [Pandas](https://examples.dask.org/dataframe.html) -> DaskDataFrame\n",
    "- [Numpy](https://examples.dask.org/array.html) -> DaskArray\n",
    "- [xarray](https://examples.dask.org/xarray.html)\n",
    "- [Iris](https://scitools.org.uk/iris/docs/latest/userguide/real_and_lazy_data.html)\n",
    "- [etc](https://dask.org/)...\n",
    "\n",
    "# [Distributed](https://distributed.dask.org/en/latest/)\n",
    "\n",
    "This is another Python library which works with Dask to interface with and execute processing tasks on cluster-like computer resource e.g. HPC, analysis cluster, cloud computing. It does this by helping dask interface with orchestration software e.g. jobqueue scheculers like [SLURM](https://github.com/dask/dask-jobqueue), [Kubernetes](https://kubernetes.dask.org/en/latest/), [Tensorflow](https://github.com/dask/dask-tensorflow).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Starting a cluster and accessing it through a client\n",
    "\n",
    "First we need to import the right libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import distributed\n",
    "import dask\n",
    "from dask_kubernetes import KubeCluster\n",
    "from dask import array as da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Cluster\n",
    "\n",
    "Here we are going to create a Kubernetes cluster on our cloud based platform (using `KubeCluster`). Kubernetes will automatically request and set up the resources needed.<br>\n",
    "The cluster will be adaptive, which means that it will scale up the number of workers when needed and scale down when fewer workers are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = KubeCluster()\n",
    "cluster.adapt()\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clicking on the **`Dashboard`** url will open a dashboard in a new browser tab. Here you can monitor the state of your dask workers and progress of an calculations.\n",
    "\n",
    "![Dask dashboard](images/dask_dashboard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect a distributed client\n",
    "\n",
    "Not all types of clusters return a useful information widget like a `KubeCluster`. For those we can pass the cluster object to a `distributed.Client`, which will render us useful things like number of workers in the cluster, but also tools to control how many workers are in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = distributed.Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our cluster with a random array\n",
    "\n",
    "To test that our cluster works (and to see it in action) we can create a `DaskArray` of random numbers and calculate its mean.\n",
    "\n",
    "The following code creates a **2500x2500x2500** array of random numbers, represented as many numpy arrays of size **500x500** (or smaller if the array cannot be divided evenly).<br>\n",
    "In this case there are **125 (5x5x5)** numpy arrays of size **500x500**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "x = da.random.random((2500, 2500, 2500), chunks=(500, 500, 500))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use standard NumPy syntax to calculate the mean of the array `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = x.mean()\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call `.compute()` when you want to start the calculation.\n",
    "\n",
    "You may want to watch the status page during computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demonstrates 'lazy' nature of this calculation - we stated all the computations we wanted to perform (`define array -> calculate mean`) but nothing is calculated until we ask dask to do so (`.compute`) or dask has to (e.g. because we have generated a plot of the data)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
