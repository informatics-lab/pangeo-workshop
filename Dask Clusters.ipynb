{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/dask_logo.svg\" alt=\"Dask\" style=\"height: 250px;\"/> \n",
    "\n",
    "# [Dask](https://docs.dask.org/en/latest/)\n",
    "\n",
    "Dask is a Python library which allows large data processing tasks to be automatically broken up into chunks and executed on a cluster of workers.\n",
    "\n",
    "Dask is integrated with many Python tools used for data processing such as:\n",
    "- [Pandas](https://examples.dask.org/dataframe.html)\n",
    "- [Numpy](https://examples.dask.org/array.html)\n",
    "- [xarray](https://examples.dask.org/xarray.html)\n",
    "- [Iris](https://scitools.org.uk/iris/docs/latest/userguide/real_and_lazy_data.html)\n",
    "- [etc](https://dask.org/)...\n",
    "\n",
    "# [Distributed](https://distributed.dask.org/en/latest/)\n",
    "\n",
    "This is another Python library which works with Dask to interface with and execute processing tasks on cluster-like computer resource e.g. HPC, analysis cluster, cloud computing. It does this by helping dask interface with orchestration software e.g. jobqueue scheculers like [SLURM](https://github.com/dask/dask-jobqueue), [Kubernetes](https://kubernetes.dask.org/en/latest/), [Tensorflow](https://github.com/dask/dask-tensorflow).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Start Dask Cluster and client\n",
    "\n",
    "We need to start by making a worker template based on our user and environment details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import distributed\n",
    "import dask\n",
    "from dask_kubernetes import KubeCluster\n",
    "from dask import array as da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select the conda environment you wish to use from above and asign to the variable below.\n",
    "It should match the environment of the notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = \"datasci\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/etc/daskernetes/worker-template.yaml') as fp:\n",
    "    template = fp.read().replace('{CONDA_DEFAULT_ENV}', env).replace(\"{JUPYTERHUB_USER}\",os.environ[\"JUPYTERHUB_USER\"])\n",
    "    template_file = f'./{env}-worker-template.yaml'\n",
    "    with open(template_file,'w') as ofp:\n",
    "        ofp.write(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = KubeCluster.from_yaml(template_file)\n",
    "cluster\n",
    "cluster.adapt() # Auto scaleing\n",
    "# cluster.scale(10) # or manual\n",
    "\n",
    "from IPython.core.display import Markdown\n",
    "port = cluster.dashboard_link.split(':')[-1]\n",
    "url = f\"https://spaceapps.informaticslab.co.uk/user/{os.environ['JUPYTERHUB_USER']}/proxy/{port}\"\n",
    "\n",
    "display(cluster)\n",
    "Markdown(f\"**The dashboard is at: [{url}]({url})**\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect a distributed client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = distributed.Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Random array\n",
    "\n",
    "This creates a 10000x10000 array of random numbers, represented as many numpy arrays of size 1000x1000 (or smaller if the array cannot be divided evenly). In this case there are 100 (10x10) numpy arrays of size 1000x1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "x = da.random.random((3000, 3000, 3000), chunks=(500, 500, 500))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use NumPy syntax as usual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call `.compute()` when you want your result as a NumPy array.\n",
    "\n",
    "You may want to watch the status page during computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = x.mean()\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
